{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import\n",
    "import math as math\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "import json\n",
    "from operator import itemgetter\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup and load input text\n",
    "SEQ_LENGTH = 50\n",
    "BATCH_SIZE = 30\n",
    "LAYER_NUM = 2\n",
    "HIDDEN_DIM = 500\n",
    "GENERATE_LENGTH = 500\n",
    "NB_EPOCH = 140 # standard:20\n",
    "GENERATE_FREQ=10 #generate text every X epoch\n",
    "MODE = \"train\"\n",
    "WEIGHTS = \"\"\n",
    "\n",
    "data_dir = 'resources/LOTR.txt'\n",
    "text = open(data_dir, \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of input text 2470966\n",
      "number of deleted chars 4360\n",
      "number of chars 2466606\n",
      "length of dictionary 60\n"
     ]
    }
   ],
   "source": [
    "#convert all data to lowecase and skip unknown chars\n",
    "data = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    t = text[i]\n",
    "    if t.isalpha() or \\\n",
    "                    t == \".\" or \\\n",
    "                    t == \",\" or \\\n",
    "                    t == \"!\" or \\\n",
    "                    t == \"?\" or \\\n",
    "                    t == \":\" or \\\n",
    "                    t == \"-\" or \\\n",
    "                    t == \" \" or \\\n",
    "                    t == \"'\":\n",
    "        data.append(t)\n",
    "\n",
    "ch = Counter(data)\n",
    "ch = collections.OrderedDict(sorted(ch.items(), key=itemgetter(1), reverse=True))\n",
    "\n",
    "#output dictionaries\n",
    "gen_ix_to_char = {ix: char for ix, char in enumerate(ch)}\n",
    "char_to_ix = {char: ix for ix, char in enumerate(ch)}\n",
    "\n",
    "text_ix = [char_to_ix[value] for value in data] \n",
    "\n",
    "print(\"number of input text\", len(text))\n",
    "print(\"number of deleted chars\", len(text) - len(data))\n",
    "print(\"number of chars\", len(data))\n",
    "print(\"length of dictionary\", len(gen_ix_to_char))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 2466606 chars\n",
      "Vocabulary size: 60 chars\n",
      "for[0:49332]\n"
     ]
    }
   ],
   "source": [
    "#prepare zero matrixes for each sequence\n",
    "VOCAB_SIZE = int(len(gen_ix_to_char))\n",
    "print('Data length: {} chars'.format(len(text_ix)))\n",
    "print('Vocabulary size: {} chars'.format(VOCAB_SIZE))\n",
    "num_of_seq=int(math.ceil(len(text_ix) / SEQ_LENGTH))\n",
    "X = np.zeros((num_of_seq, SEQ_LENGTH, VOCAB_SIZE))\n",
    "y = np.zeros((num_of_seq, SEQ_LENGTH, VOCAB_SIZE))\n",
    "\n",
    "print(\"for[0:\" + str(math.floor(len(text_ix) / SEQ_LENGTH)) + \"]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 49332\n",
      "5000 / 49332\n",
      "10000 / 49332\n",
      "15000 / 49332\n",
      "20000 / 49332\n",
      "25000 / 49332\n",
      "30000 / 49332\n",
      "35000 / 49332\n",
      "40000 / 49332\n",
      "45000 / 49332\n"
     ]
    }
   ],
   "source": [
    "#function for text generation\n",
    "range_num=int(math.floor(len(text_ix) / SEQ_LENGTH))\n",
    "for i in range(range_num):\n",
    "        if i % 5000 == 0: print(i, '/', range_num)\n",
    "        X_sequence_ix = text_ix[i * SEQ_LENGTH:(i + 1) * SEQ_LENGTH]\n",
    "        input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "\n",
    "        for j in range(SEQ_LENGTH):\n",
    "            input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "            X[i] = input_sequence\n",
    "\n",
    "        y_sequence_ix = text_ix[i * SEQ_LENGTH + 1:(i + 1) * SEQ_LENGTH + 1]\n",
    "        target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "        for j in range(SEQ_LENGTH):\n",
    "            target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "            y[i] = target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating and compiling the Network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function for text generation\n",
    "def generate_text(model, length, vocab_size, gen_ix_to_char):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    # print(ix)\n",
    "    y_char = [gen_ix_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(gen_ix_to_char[ix[-1]], sep='', end='', flush=True)\n",
    "\n",
    "        ix = np.argmax(model.predict(X[:, :i + 1, :])[0], 1)\n",
    "        y_char.append(gen_ix_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBATFaFadTDfT--T:p:::f:::OOOOOOOOy!!!!!IIIIj--gwRwwqqqgEEEEgEyNkCCppppEEE!!CGGNBBABNTATFFppppEEE!!!GGNNBBAATFTTpppwwwqqqqEEgEEEk!CCkssssksYYYYYtttttIIIIIwwwYYqtqqggEEEENk'kypppEEE!CCCGGNBBBABTTT,,vvwwwwqqqqqgEEEEEk'ypppEEECCCCpGpGBBBBZZNBBZZNNBKKKKKZZNYYYtttttKKKKKKrYYYtttttIIIIwwYYttttIIppwIwIwRqqqEEgEENkkksssskssksssksYYYYYtttttIIIIIwwwYYqtqqggEEEENk'kypppEEE!CCCGGNBBBABTTT,,vvwwwwqqqqqgEEEEEk'ypppEEECCCCpGpGBBBBZZNBBZZNNBKKKKKZZNYYYtttttKKKKKKrYYYtttttIIIIwwYYttttIIppwIwIwRqqqEEgEENkkksssskss"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"NBATFaFadTDfT--T:p:::f:::OOOOOOOOy!!!!!IIIIj--gwRwwqqqgEEEEgEyNkCCppppEEE!!CGGNBBABNTATFFppppEEE!!!GGNNBBAATFTTpppwwwqqqqEEgEEEk!CCkssssksYYYYYtttttIIIIIwwwYYqtqqggEEEENk'kypppEEE!CCCGGNBBBABTTT,,vvwwwwqqqqqgEEEEEk'ypppEEECCCCpGpGBBBBZZNBBZZNNBKKKKKZZNYYYtttttKKKKKKrYYYtttttIIIIwwYYttttIIppwIwIwRqqqEEgEENkkksssskssksssksYYYYYtttttIIIIIwwwYYqtqqggEEEENk'kypppEEE!CCCGGNBBBABTTT,,vvwwwwqqqqqgEEEEEk'ypppEEECCCCpGpGBBBBZZNBBZZNNBKKKKKZZNYYYtttttKKKKKKrYYYtttttIIIIwwYYttttIIppwIwIwRqqqEEgEENkkksssskssk\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate some sample before training to know how bad it is!\n",
    "model.save('checkpoint_layer_{}_hidden_{}_test.hdf5'.format(LAYER_NUM, HIDDEN_DIM))\n",
    "generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, gen_ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 0\n",
      "\n",
      "Epoch 1/1\n",
      " 9150/49333 [====>.........................] - ETA: 8:58 - loss: 2.5585"
     ]
    }
   ],
   "source": [
    "#Training if there is no trained weights specified\n",
    "nb_epoch = 0\n",
    "if MODE == 'train' or WEIGHTS == '':\n",
    "    while nb_epoch<NB_EPOCH:\n",
    "        print('\\n\\nEpoch: {}\\n'.format(nb_epoch))\n",
    "        model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=1)\n",
    "        nb_epoch += 1\n",
    "\n",
    "\n",
    "        # generate every GENERATE_FREQth epoch\n",
    "        if nb_epoch % GENERATE_FREQ == 0 or nb_epoch==1 or nb_epoch==5 or nb_epoch==10:\n",
    "            gen_text = generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, gen_ix_to_char)\n",
    "            gen_text = str(gen_text)\n",
    "           \n",
    "            with open('char_{}_hidden_{}_epoch_{}.json'.format(LAYER_NUM, HIDDEN_DIM, nb_epoch), 'w') as fp:\n",
    "                json.dump(gen_text, fp)\n",
    "\n",
    "            model.save_weights('char_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, nb_epoch))\n",
    "            # model.save('checkpoint_layer_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, nb_epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
