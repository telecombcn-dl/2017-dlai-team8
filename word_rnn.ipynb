{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import\n",
    "import re\n",
    "import math as math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup and load input text\n",
    "TEXT_LENGTH=1000000 #try less in a case of memory error\n",
    "SEQ_LENGTH = 10\n",
    "BATCH_SIZE = 30\n",
    "LAYER_NUM = 2\n",
    "HIDDEN_DIM = 500\n",
    "GENERATE_LENGTH = 50\n",
    "NB_EPOCH = 140 # standard:20\n",
    "GENERATE_FREQ=10 #generate text every X epoch\n",
    "FREQ_LIMIT=1 # words with occurency up to the value will be counted and renamed as unknown\n",
    "\n",
    "data_dir = 'resources/LOTR.txt'\n",
    "text = open(data_dir, \"r\").read()\n",
    "text = text[0:TEXT_LENGTH] #just 1,000,000 because of memory limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert input text word by word\n",
    "wordRegEx = re.compile(\"(?:[a-zA-Z]+)|(?:[,;:.!'Â´?])\")\n",
    "words = wordRegEx.findall(text)\n",
    "\n",
    "data=[x.lower() for x in words]\n",
    "\n",
    "# have two sorted arrays - one will be reduced by FREQ_LIMIT to get final dict for generating text, \n",
    "# second one will be in origine size for translation of input text\n",
    "words_to_freq=sorted(Counter(data).items(), key=lambda x: x[1], reverse=True)\n",
    "words_to_freq2=sorted(Counter(data).items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DICTIONARY FOR TEXT GENERATION - with word unknown\n",
    "sum_of_freqs = 0\n",
    "x = 0\n",
    "while x < len(words_to_freq):\n",
    "    if words_to_freq[x][1] <= FREQ_LIMIT:\n",
    "        # words_to_freq2[x][0]=\"unknown\"\n",
    "        sum_of_freqs += words_to_freq[x][1]\n",
    "\n",
    "        del words_to_freq[x]\n",
    "    x += 1\n",
    "\n",
    "words_to_freq.append(tuple(('unknown', sum_of_freqs)))\n",
    "words_to_freq = sorted(words_to_freq, key=lambda x: x[1], reverse=True)\n",
    "words_sorted = [x[0] for x in words_to_freq]\n",
    "gen_ix_to_word = {ix: char for ix, char in enumerate(words_sorted)}  # output dictionary for translating input text-> idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input text 500000\n",
      "Number of words 114065\n",
      "Unique words 4854\n",
      "Deleted words 1257\n",
      "Vocabulary size: 6112\n"
     ]
    }
   ],
   "source": [
    "# DICTIONARY WITH SUM of OCCURRENCE\n",
    "textDict = []\n",
    "for x in range(len(words_to_freq2)):\n",
    "    if words_to_freq2[x][1] <= FREQ_LIMIT:\n",
    "        textDict.append([words_to_freq2[x][0], sum_of_freqs])\n",
    "    else:\n",
    "        textDict.append(words_to_freq2[x])\n",
    "\n",
    "textDict = sorted(textDict, key=lambda x: x[1], reverse=True)\n",
    "ix_to_word = []\n",
    "cnt = 0\n",
    "for x in range(len(textDict)):  #making of indexes for word_to_number dictionary for text generation\n",
    "    if textDict[x][1] != sum_of_freqs:\n",
    "        cnt += 1\n",
    "    ix_to_word.append([textDict[x][0], cnt])\n",
    "\n",
    "ix_to_word = {t[0]: t[1] for t in ix_to_word}\n",
    "\n",
    "# translated text\n",
    "text_ix = [ix_to_word[value] for value in data]\n",
    "print(\"Length of input text\", len(text))\n",
    "print(\"Number of words\", len(words))\n",
    "print(\"Unique words\", len(gen_ix_to_word))\n",
    "print(\"Deleted words\", len(ix_to_word) - len(gen_ix_to_word) - 1)\n",
    "print(\"Vocabulary size:\", len(ix_to_word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for[0:11406]\n"
     ]
    }
   ],
   "source": [
    "#prepare zero matrixes for each sequence\n",
    "VOCAB_SIZE = int(len(gen_ix_to_word))\n",
    "num_of_seq = int(math.ceil(len(text_ix) / SEQ_LENGTH))\n",
    "X = np.zeros((num_of_seq, SEQ_LENGTH, VOCAB_SIZE))\n",
    "y = np.zeros((num_of_seq, SEQ_LENGTH, VOCAB_SIZE))\n",
    "\n",
    "print(\"for[0:\" + str(math.floor(len(text_ix) / SEQ_LENGTH)) + \"]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 11406\n",
      "5000 / 11406\n",
      "10000 / 11406\n"
     ]
    }
   ],
   "source": [
    "#fill the matrixes\n",
    "range_num = int(math.floor(len(text_ix) / SEQ_LENGTH))\n",
    "for i in range(range_num):\n",
    "    if i % 5000 == 0: print(i, '/', range_num)\n",
    "    X_sequence_ix = text_ix[i * SEQ_LENGTH:(i + 1) * SEQ_LENGTH]\n",
    "    input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "        X[i] = input_sequence\n",
    "    y_sequence_ix = text_ix[i * SEQ_LENGTH + 1:(i + 1) * SEQ_LENGTH + 1]\n",
    "\n",
    "    target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "        y[i] = target_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------Creating and compiling the Network------/\n"
     ]
    }
   ],
   "source": [
    "#Creating and compiling the Network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function for text generation\n",
    "def generate_text(model, length, vocab_size, gen_ix_to_word):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(vocab_size)]\n",
    "    print(ix)\n",
    "    y_char = [gen_ix_to_word[ix[-1]]]\n",
    "    X = np.zeros((1, length, vocab_size))\n",
    "    upper = True\n",
    "    for i in range(length):\n",
    "        # appending the last predicted character to sequence\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        word = gen_ix_to_word[ix[-1]]\n",
    "\n",
    "        if upper:\n",
    "            word = word.title()\n",
    "\n",
    "        if word == \".\" or word == \"?\" or word == \"!\":\n",
    "            upper = True\n",
    "        else:\n",
    "            upper = False\n",
    "\n",
    "        print(word + ' ',end=\"\",flush=True)\n",
    "        ix = np.argmax(model.predict(X[:, :i + 1, :])[0], 1)\n",
    "        y_char.append(word)\n",
    "    return (' ').join(y_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------Generate some sample before training to know how bad it is!------\n",
      "[1831]\n",
      "Weeks hammered across fence easy easy fence anyway courage courage ring departure ring decide decide foolish foolish foolish gum heartening path path beneath beneath path beneath dawn dawn sawed sawed sawed fading fading fading whom never never apparently apparently apparently covered covered gown gown once once branch dies she she "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'weeks Weeks hammered across fence easy easy fence anyway courage courage ring departure ring decide decide foolish foolish foolish gum heartening path path beneath beneath path beneath dawn dawn sawed sawed sawed fading fading fading whom never never apparently apparently apparently covered covered gown gown once once branch dies she she'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate some sample before training to know how bad it is!\n",
    "generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, gen_ix_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------Training if there is no trained weights specified------\n",
      "\n",
      "\n",
      "Epoch: 0\n",
      "\n",
      "Epoch 1/1\n",
      " 1560/11407 [===>..........................] - ETA: 1:47 - loss: 6.4571"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "nb_epoch = 0\n",
    "\n",
    "while nb_epoch < NB_EPOCH:\n",
    "    print('\\n\\nEpoch: {}\\n'.format(nb_epoch))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=1)\n",
    "    nb_epoch += 1\n",
    "\n",
    "    # generate every GENERATE_FREQth epoch\n",
    "    if nb_epoch % GENERATE_FREQ == 0 or nb_epoch == 1 or nb_epoch == 5 or nb_epoch == 10:\n",
    "        gen_text = generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, gen_ix_to_word)\n",
    "        space = \" \"\n",
    "\n",
    "        gen_text = [space, nb_epoch, space, str(gen_text)]\n",
    "        text.append(gen_text)\n",
    "        with open('char_{}_hidden_{}_epoch_{}.json'.format(LAYER_NUM, HIDDEN_DIM, nb_epoch), 'w') as fp:\n",
    "            json.dump(text, fp)\n",
    "\n",
    "        model.save_weights('char_{}_hidden_{}_epoch_{}.hdf5'.format(LAYER_NUM, HIDDEN_DIM, nb_epoch))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
